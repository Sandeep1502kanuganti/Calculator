{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUCVBk4rBbv/CsJ2GQAMF+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sandeep1502kanuganti/Calculator/blob/main/ids.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DdC9mB8VKbx6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13cd9984"
      },
      "source": [
        "# Task\n",
        "Develop an intrusion detection system using deep learning, encompassing data collection and preprocessing, model development, training, evaluation, and explanation of results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34c0f659"
      },
      "source": [
        "## Data Collection and Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Acquire and preprocess a suitable dataset for intrusion detection, which typically involves cleaning, normalization, and feature engineering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2ffe63b"
      },
      "source": [
        "```markdown\n",
        "## Data Acquisition\n",
        "\n",
        "For this intrusion detection task, we will use the **CICIDS2017** dataset. This dataset is a modern and comprehensive benchmark for intrusion detection systems, containing benign and the most up-to-date common attacks. It includes network traffic captured in a realistic environment with labeled flows, which is ideal for our purpose.\n",
        "\n",
        "We will download the dataset from its official source. The dataset is split into several CSV files, one for each day of traffic capture. We will download the first day's traffic data for demonstration.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feff3784"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step introduced the dataset. Now, following the first instruction of the subtask, I will download the specified CICIDS2017 dataset (specifically, the Friday-WorkingHours-Afternoon-PortScan data) from its URL using a shell command.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71112884",
        "outputId": "ce2ff926-8d9a-44fe-e30f-329e5076a68f"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the URL for the CICIDS2017 dataset (Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv)\n",
        "dataset_url = \"https://www.unb.ca/cic/datasets/cicids2017-csv.html\"\n",
        "# Direct link to the Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv file if available, or instruct user to download manually if direct link not available through simple wget\n",
        "# For simplicity and to avoid complex web scraping, let's assume we have a direct link or instruct on manual download.\n",
        "# As the original dataset link points to an HTML page with multiple downloads, we'll pick one directly from a known mirror/source if possible, or illustrate the process.\n",
        "\n",
        "# For demonstration, let's use a specific day's CSV from a common mirror or re-host.\n",
        "# The actual CICIDS2017 dataset is large and split. Let's pick one file for initial processing.\n",
        "# A direct download link for one of the CSV files is needed. Searching for a public direct link...\n",
        "# For example, Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
        "\n",
        "# Using wget for direct download of one of the files from a known source if available\n",
        "file_name = \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\"\n",
        "# NOTE: This URL is an example. The actual direct download link for CICIDS2017 CSV files can be hard to find and often changes.\n",
        "# Users might need to manually download from the UNB website or use a different source.\n",
        "# For a reproducible example, let's use a known public dataset if CICIDS2017 direct link is problematic.\n",
        "# For now, let's assume a direct link exists and use a placeholder. Or better, use a smaller, readily available dataset if CICIDS2017 is too complex for direct download in a single step.\n",
        "\n",
        "# Let's use a common public mirror for one of the CICIDS2017 files for direct download for this exercise.\n",
        "# Example: 'https://cdn.unb.ca/cicids2017/CSV-f/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv'\n",
        "# This specific link might not work directly or exist. It's better to guide the user to download it.\n",
        "\n",
        "# Let's adjust to a more robust approach if direct 'wget' to a CSV isn't straightforward from the main page.\n",
        "# For Colab, often Google Drive links or specific pre-uploaded datasets are used.\n",
        "\n",
        "# Instead of downloading from the official page, which requires parsing HTML, we can provide a direct link from a known source if available.\n",
        "# Let's use a direct link for a portion of the dataset if possible.\n",
        "# A simple way to get one of the CICIDS2017 files if hosted directly somewhere.\n",
        "\n",
        "# Option 1: Use a publicly available smaller dataset if CICIDS2017 direct download is too complex for automated single-step download.\n",
        "# Option 2: Guide the user to download manually.\n",
        "# Option 3: Find a reliable direct link to one of the CSV files.\n",
        "\n",
        "# Given the prompt, let's stick to CICIDS2017 and try to find a direct link for one of the days.\n",
        "# After a quick search, direct links like 'http://205.174.165.80/CICDataset/CIC-IDS-2017/CSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv'\n",
        "# are sometimes available but can be unstable.\n",
        "\n",
        "# Let's try to use a command line approach to download one of the days.\n",
        "# The file 'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv' is a good candidate.\n",
        "# Using a known direct link if one exists. For this example, let's assume one of the provided links from UNB works.\n",
        "# Note: Google Colab often has issues with direct `wget` from some UNB servers due to redirection or security.\n",
        "\n",
        "# Let's try with a common workaround for Colab, using `gdown` if the dataset is on Google Drive, or `!wget` with appropriate headers/redirections.\n",
        "# Since CICIDS2017 is not typically on Google Drive, `!wget` is the way.\n",
        "\n",
        "print(\"Attempting to download 'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv' from a direct URL.\")\n",
        "print(\"Please note: Direct download links for CICIDS2017 can be unreliable. If this fails, consider manual download.\")\n",
        "\n",
        "# Using an archived link that might be more stable for programmatic download\n",
        "direct_download_url = \"https://web.archive.org/web/20210815045053/http://205.174.165.80/CICDataset/CIC-IDS-2017/CSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\"\n",
        "\n",
        "# Use !wget to download the file\n",
        "!wget -O {file_name} \"{direct_download_url}\"\n",
        "\n",
        "print(f\"Downloaded {file_name}.\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to download 'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv' from a direct URL.\n",
            "Please note: Direct download links for CICIDS2017 can be unreliable. If this fails, consider manual download.\n",
            "--2025-11-17 19:04:37--  https://web.archive.org/web/20210815045053/http://205.174.165.80/CICDataset/CIC-IDS-2017/CSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
            "Resolving web.archive.org (web.archive.org)... 207.241.237.3\n",
            "Connecting to web.archive.org (web.archive.org)|207.241.237.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 NOT FOUND\n",
            "2025-11-17 19:04:38 ERROR 404: NOT FOUND.\n",
            "\n",
            "Downloaded Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bec6a100"
      },
      "source": [
        "```markdown\n",
        "## Data Acquisition (Correction)\n",
        "\n",
        "The previous attempt to download the CICIDS2017 dataset failed due to a 404 error, indicating the direct download link was no longer valid. Datasets like CICIDS2017 often have volatile direct links.\n",
        "\n",
        "To proceed, we will instead acquire the **NSL-KDD** dataset. NSL-KDD is a widely used benchmark dataset for intrusion detection systems, derived from the KDD Cup 1999 dataset, addressing some of its inherent problems. It provides distinct training and testing sets, which is beneficial for model evaluation.\n",
        "\n",
        "We will download the `KDDTrain+.txt` and `KDDTest+.txt` files, which contain the training and testing data respectively, from a public repository.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef601e15"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the previous markdown block's explanation, I will now download the `KDDTrain+.txt` and `KDDTest+.txt` files for the NSL-KDD dataset using `wget` from a known public repository.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "902151f8",
        "outputId": "4afb4263-c95e-4b46-a143-1b8c6203d396"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the file names\n",
        "train_file_name = \"KDDTrain+.txt\"\n",
        "test_file_name = \"KDDTest+.txt\"\n",
        "\n",
        "# Define the direct download URLs for NSL-KDD training and test sets\n",
        "# These URLs are from a common repository for NSL-KDD\n",
        "train_url = \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\" # Often used as training or a sample\n",
        "# The actual KDDTrain+.txt and KDDTest+.txt are derived from the full KDD Cup 99 or specific NSL-KDD versions.\n",
        "# Let's use direct links for NSL-KDD from a more reliable source often used in research.\n",
        "\n",
        "# Reliable source for NSL-KDD files (e.g., from github repositories or UCI directly if available)\n",
        "# A common source for these files for direct download:\n",
        "# KDDTrain+.txt: https://raw.githubusercontent.com/jbrownlee/Datasets/master/NSL-KDD/KDDTrain%2B.arff\n",
        "# KDDTest+.txt: https://raw.githubusercontent.com/jbrownlee/Datasets/master/NSL-KDD/KDDTest%2B.arff\n",
        "\n",
        "# However, these are ARFF files. We need TXT or CSV for easier processing.\n",
        "# Let's find TXT/CSV versions.\n",
        "\n",
        "# Using a known public repository that hosts these as text files:\n",
        "# Source: https://github.com/defcom17/NSL_KDD\n",
        "nsl_kdd_base_url = \"https://raw.githubusercontent.com/defcom17/NSL_KDD/master/\"\n",
        "train_dataset_url = nsl_kdd_base_url + \"KDDTrain%2B.txt\"\n",
        "test_dataset_url = nsl_kdd_base_url + \"KDDTest%2B.txt\"\n",
        "\n",
        "print(f\"Attempting to download {train_file_name} and {test_file_name} for NSL-KDD.\")\n",
        "\n",
        "# Download KDDTrain+.txt\n",
        "!wget -O {train_file_name} \"{train_dataset_url}\"\n",
        "print(f\"Downloaded {train_file_name}.\")\n",
        "\n",
        "# Download KDDTest+.txt\n",
        "!wget -O {test_file_name} \"{test_dataset_url}\"\n",
        "print(f\"Downloaded {test_file_name}.\")\n",
        "\n",
        "# Verify files are downloaded\n",
        "if os.path.exists(train_file_name) and os.path.exists(test_file_name):\n",
        "    print(\"NSL-KDD datasets downloaded successfully.\")\n",
        "else:\n",
        "    print(\"Error: One or more NSL-KDD dataset files could not be downloaded.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to download KDDTrain+.txt and KDDTest+.txt for NSL-KDD.\n",
            "--2025-11-17 19:04:38--  https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain%2B.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19109424 (18M) [text/plain]\n",
            "Saving to: ‘KDDTrain+.txt’\n",
            "\n",
            "KDDTrain+.txt       100%[===================>]  18.22M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-11-17 19:04:38 (241 MB/s) - ‘KDDTrain+.txt’ saved [19109424/19109424]\n",
            "\n",
            "Downloaded KDDTrain+.txt.\n",
            "--2025-11-17 19:04:39--  https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTest%2B.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3441513 (3.3M) [text/plain]\n",
            "Saving to: ‘KDDTest+.txt’\n",
            "\n",
            "KDDTest+.txt        100%[===================>]   3.28M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-11-17 19:04:39 (108 MB/s) - ‘KDDTest+.txt’ saved [3441513/3441513]\n",
            "\n",
            "Downloaded KDDTest+.txt.\n",
            "NSL-KDD datasets downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8f8c16b"
      },
      "source": [
        "```markdown\n",
        "## Load NSL-KDD Dataset into Pandas DataFrames\n",
        "\n",
        "Now that the NSL-KDD dataset files (`KDDTrain+.txt` and `KDDTest+.txt`) have been successfully downloaded, the next step is to load them into pandas DataFrames. These files are typically comma-separated and do not contain a header row. Therefore, we need to explicitly define the column names.\n",
        "\n",
        "The NSL-KDD dataset has 41 features and one class label. The features include basic features of individual TCP connections, content features within a connection, and time-based and host-based traffic features. The last column is the class label, indicating whether the connection is `normal` or an `attack` type.\n",
        "\n",
        "We will define a list of column names, then load both the training and testing datasets using `pd.read_csv`.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afe049b5"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the explanation in the previous markdown block, I will now define the column names for the NSL-KDD dataset and load both the training and testing text files into pandas DataFrames using `pd.read_csv`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e212ddf7",
        "outputId": "4d36c674-8117-461f-8a43-ce66316d59b3"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Column names for the NSL-KDD dataset (41 features + 1 label)\n",
        "# Source for column names: http://www.unb.ca/cic/datasets/nsl-kdd.html\n",
        "# Or often derived from kddcup.names which is for KDD'99, but NSL-KDD shares similar features.\n",
        "# Let's define them based on common NSL-KDD usage.\n",
        "column_names = [\n",
        "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
        "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
        "    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
        "    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
        "    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
        "    'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
        "    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
        "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
        "    'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
        "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'class', 'difficulty_level' # difficulty_level is an extra column in some NSL-KDD versions, we will drop it later if needed\n",
        "]\n",
        "\n",
        "# Load the training dataset\n",
        "train_df = pd.read_csv(train_file_name, sep=',', names=column_names)\n",
        "print(f\"Loaded {train_file_name}. Shape: {train_df.shape}\")\n",
        "\n",
        "# Load the testing dataset\n",
        "test_df = pd.read_csv(test_file_name, sep=',', names=column_names)\n",
        "print(f\"Loaded {test_file_name}. Shape: {test_df.shape}\")\n",
        "\n",
        "# Display the first few rows of the training DataFrame\n",
        "print(\"\\nFirst 5 rows of training data:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# Display the first few rows of the testing DataFrame\n",
        "print(\"\\nFirst 5 rows of testing data:\")\n",
        "print(test_df.head())\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded KDDTrain+.txt. Shape: (125973, 43)\n",
            "Loaded KDDTest+.txt. Shape: (22544, 43)\n",
            "\n",
            "First 5 rows of training data:\n",
            "   duration protocol_type   service flag  src_bytes  dst_bytes  land  \\\n",
            "0         0           tcp  ftp_data   SF        491          0     0   \n",
            "1         0           udp     other   SF        146          0     0   \n",
            "2         0           tcp   private   S0          0          0     0   \n",
            "3         0           tcp      http   SF        232       8153     0   \n",
            "4         0           tcp      http   SF        199        420     0   \n",
            "\n",
            "   wrong_fragment  urgent  hot  ...  dst_host_same_srv_rate  \\\n",
            "0               0       0    0  ...                    0.17   \n",
            "1               0       0    0  ...                    0.00   \n",
            "2               0       0    0  ...                    0.10   \n",
            "3               0       0    0  ...                    1.00   \n",
            "4               0       0    0  ...                    1.00   \n",
            "\n",
            "   dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
            "0                    0.03                         0.17   \n",
            "1                    0.60                         0.88   \n",
            "2                    0.05                         0.00   \n",
            "3                    0.00                         0.03   \n",
            "4                    0.00                         0.00   \n",
            "\n",
            "   dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
            "0                         0.00                  0.00   \n",
            "1                         0.00                  0.00   \n",
            "2                         0.00                  1.00   \n",
            "3                         0.04                  0.03   \n",
            "4                         0.00                  0.00   \n",
            "\n",
            "   dst_host_srv_serror_rate  dst_host_rerror_rate  dst_host_srv_rerror_rate  \\\n",
            "0                      0.00                  0.05                      0.00   \n",
            "1                      0.00                  0.00                      0.00   \n",
            "2                      1.00                  0.00                      0.00   \n",
            "3                      0.01                  0.00                      0.01   \n",
            "4                      0.00                  0.00                      0.00   \n",
            "\n",
            "     class  difficulty_level  \n",
            "0   normal                20  \n",
            "1   normal                15  \n",
            "2  neptune                19  \n",
            "3   normal                21  \n",
            "4   normal                21  \n",
            "\n",
            "[5 rows x 43 columns]\n",
            "\n",
            "First 5 rows of testing data:\n",
            "   duration protocol_type   service  flag  src_bytes  dst_bytes  land  \\\n",
            "0         0           tcp   private   REJ          0          0     0   \n",
            "1         0           tcp   private   REJ          0          0     0   \n",
            "2         2           tcp  ftp_data    SF      12983          0     0   \n",
            "3         0          icmp     eco_i    SF         20          0     0   \n",
            "4         1           tcp    telnet  RSTO          0         15     0   \n",
            "\n",
            "   wrong_fragment  urgent  hot  ...  dst_host_same_srv_rate  \\\n",
            "0               0       0    0  ...                    0.04   \n",
            "1               0       0    0  ...                    0.00   \n",
            "2               0       0    0  ...                    0.61   \n",
            "3               0       0    0  ...                    1.00   \n",
            "4               0       0    0  ...                    0.31   \n",
            "\n",
            "   dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
            "0                    0.06                         0.00   \n",
            "1                    0.06                         0.00   \n",
            "2                    0.04                         0.61   \n",
            "3                    0.00                         1.00   \n",
            "4                    0.17                         0.03   \n",
            "\n",
            "   dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
            "0                         0.00                   0.0   \n",
            "1                         0.00                   0.0   \n",
            "2                         0.02                   0.0   \n",
            "3                         0.28                   0.0   \n",
            "4                         0.02                   0.0   \n",
            "\n",
            "   dst_host_srv_serror_rate  dst_host_rerror_rate  dst_host_srv_rerror_rate  \\\n",
            "0                       0.0                  1.00                      1.00   \n",
            "1                       0.0                  1.00                      1.00   \n",
            "2                       0.0                  0.00                      0.00   \n",
            "3                       0.0                  0.00                      0.00   \n",
            "4                       0.0                  0.83                      0.71   \n",
            "\n",
            "     class  difficulty_level  \n",
            "0  neptune                21  \n",
            "1  neptune                21  \n",
            "2   normal                21  \n",
            "3    saint                15  \n",
            "4    mscan                11  \n",
            "\n",
            "[5 rows x 43 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "574dd5d5"
      },
      "source": [
        "```markdown\n",
        "## Initial Data Exploration\n",
        "\n",
        "With the datasets loaded, we will now perform initial data exploration to gain a better understanding of their structure and content. This includes:\n",
        "\n",
        "*   **Checking data types:** To ensure columns are interpreted correctly (e.g., numerical features as numbers, categorical features as objects).\n",
        "*   **Reviewing basic statistics:** To understand the distribution and range of numerical features.\n",
        "*   **Identifying missing values:** To detect any incomplete data that needs handling in subsequent preprocessing steps.\n",
        "*   **Inspecting unique values in categorical columns:** To understand the variety within these features.\n",
        "*   **Dropping the 'difficulty_level' column:** This column is not relevant for the classification task and was added during dataset generation, not part of the actual network traffic features, so it will be removed from both training and testing datasets.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da61e9fe"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the data exploration plan, I will start by dropping the 'difficulty_level' column from both training and testing DataFrames, then proceed with checking data types, summary statistics, and missing values for both datasets to gain an initial understanding of their structure and content.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0492000d",
        "outputId": "2d0f6711-a3b0-4f09-e0dc-bdacf281cc97"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Drop 'difficulty_level' column as it's not a feature for the model\n",
        "train_df = train_df.drop('difficulty_level', axis=1)\n",
        "test_df = test_df.drop('difficulty_level', axis=1)\n",
        "print(\"Dropped 'difficulty_level' column from both datasets.\")\n",
        "print(f\"Updated training data shape: {train_df.shape}\")\n",
        "print(f\"Updated testing data shape: {test_df.shape}\")\n",
        "\n",
        "# Display basic information for training data\n",
        "print(\"\\n--- Training Data Info ---\")\n",
        "train_df.info()\n",
        "\n",
        "# Display basic information for testing data\n",
        "print(\"\\n--- Testing Data Info ---\")\n",
        "test_df.info()\n",
        "\n",
        "# Display descriptive statistics for numerical columns in training data\n",
        "print(\"\\n--- Training Data Descriptive Statistics ---\")\n",
        "print(train_df.describe())\n",
        "\n",
        "# Display descriptive statistics for numerical columns in testing data\n",
        "print(\"\\n--- Testing Data Descriptive Statistics ---\")\n",
        "print(test_df.describe())\n",
        "\n",
        "# Check for missing values in training data\n",
        "print(\"\\n--- Missing Values in Training Data ---\")\n",
        "print(train_df.isnull().sum().sort_values(ascending=False))\n",
        "\n",
        "# Check for missing values in testing data\n",
        "print(\"\\n--- Missing Values in Testing Data ---\")\n",
        "print(test_df.isnull().sum().sort_values(ascending=False))\n",
        "\n",
        "# Identify categorical columns and display unique values for initial inspection\n",
        "categorical_cols = train_df.select_dtypes(include=['object']).columns\n",
        "\n",
        "print(\"\\n--- Unique Values in Categorical Columns (Training Data) ---\")\n",
        "for col in categorical_cols:\n",
        "    print(f\"Column '{col}': {train_df[col].nunique()} unique values\")\n",
        "    print(train_df[col].value_counts())\n",
        "    print(\"\\n\")\n",
        "\n",
        "print(\"\\n--- Unique Values in Categorical Columns (Testing Data) ---\")\n",
        "for col in categorical_cols:\n",
        "    print(f\"Column '{col}': {test_df[col].nunique()} unique values\")\n",
        "    print(test_df[col].value_counts())\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped 'difficulty_level' column from both datasets.\n",
            "Updated training data shape: (125973, 42)\n",
            "Updated testing data shape: (22544, 42)\n",
            "\n",
            "--- Training Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 125973 entries, 0 to 125972\n",
            "Data columns (total 42 columns):\n",
            " #   Column                       Non-Null Count   Dtype  \n",
            "---  ------                       --------------   -----  \n",
            " 0   duration                     125973 non-null  int64  \n",
            " 1   protocol_type                125973 non-null  object \n",
            " 2   service                      125973 non-null  object \n",
            " 3   flag                         125973 non-null  object \n",
            " 4   src_bytes                    125973 non-null  int64  \n",
            " 5   dst_bytes                    125973 non-null  int64  \n",
            " 6   land                         125973 non-null  int64  \n",
            " 7   wrong_fragment               125973 non-null  int64  \n",
            " 8   urgent                       125973 non-null  int64  \n",
            " 9   hot                          125973 non-null  int64  \n",
            " 10  num_failed_logins            125973 non-null  int64  \n",
            " 11  logged_in                    125973 non-null  int64  \n",
            " 12  num_compromised              125973 non-null  int64  \n",
            " 13  root_shell                   125973 non-null  int64  \n",
            " 14  su_attempted                 125973 non-null  int64  \n",
            " 15  num_root                     125973 non-null  int64  \n",
            " 16  num_file_creations           125973 non-null  int64  \n",
            " 17  num_shells                   125973 non-null  int64  \n",
            " 18  num_access_files             125973 non-null  int64  \n",
            " 19  num_outbound_cmds            125973 non-null  int64  \n",
            " 20  is_host_login                125973 non-null  int64  \n",
            " 21  is_guest_login               125973 non-null  int64  \n",
            " 22  count                        125973 non-null  int64  \n",
            " 23  srv_count                    125973 non-null  int64  \n",
            " 24  serror_rate                  125973 non-null  float64\n",
            " 25  srv_serror_rate              125973 non-null  float64\n",
            " 26  rerror_rate                  125973 non-null  float64\n",
            " 27  srv_rerror_rate              125973 non-null  float64\n",
            " 28  same_srv_rate                125973 non-null  float64\n",
            " 29  diff_srv_rate                125973 non-null  float64\n",
            " 30  srv_diff_host_rate           125973 non-null  float64\n",
            " 31  dst_host_count               125973 non-null  int64  \n",
            " 32  dst_host_srv_count           125973 non-null  int64  \n",
            " 33  dst_host_same_srv_rate       125973 non-null  float64\n",
            " 34  dst_host_diff_srv_rate       125973 non-null  float64\n",
            " 35  dst_host_same_src_port_rate  125973 non-null  float64\n",
            " 36  dst_host_srv_diff_host_rate  125973 non-null  float64\n",
            " 37  dst_host_serror_rate         125973 non-null  float64\n",
            " 38  dst_host_srv_serror_rate     125973 non-null  float64\n",
            " 39  dst_host_rerror_rate         125973 non-null  float64\n",
            " 40  dst_host_srv_rerror_rate     125973 non-null  float64\n",
            " 41  class                        125973 non-null  object \n",
            "dtypes: float64(15), int64(23), object(4)\n",
            "memory usage: 40.4+ MB\n",
            "\n",
            "--- Testing Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 22544 entries, 0 to 22543\n",
            "Data columns (total 42 columns):\n",
            " #   Column                       Non-Null Count  Dtype  \n",
            "---  ------                       --------------  -----  \n",
            " 0   duration                     22544 non-null  int64  \n",
            " 1   protocol_type                22544 non-null  object \n",
            " 2   service                      22544 non-null  object \n",
            " 3   flag                         22544 non-null  object \n",
            " 4   src_bytes                    22544 non-null  int64  \n",
            " 5   dst_bytes                    22544 non-null  int64  \n",
            " 6   land                         22544 non-null  int64  \n",
            " 7   wrong_fragment               22544 non-null  int64  \n",
            " 8   urgent                       22544 non-null  int64  \n",
            " 9   hot                          22544 non-null  int64  \n",
            " 10  num_failed_logins            22544 non-null  int64  \n",
            " 11  logged_in                    22544 non-null  int64  \n",
            " 12  num_compromised              22544 non-null  int64  \n",
            " 13  root_shell                   22544 non-null  int64  \n",
            " 14  su_attempted                 22544 non-null  int64  \n",
            " 15  num_root                     22544 non-null  int64  \n",
            " 16  num_file_creations           22544 non-null  int64  \n",
            " 17  num_shells                   22544 non-null  int64  \n",
            " 18  num_access_files             22544 non-null  int64  \n",
            " 19  num_outbound_cmds            22544 non-null  int64  \n",
            " 20  is_host_login                22544 non-null  int64  \n",
            " 21  is_guest_login               22544 non-null  int64  \n",
            " 22  count                        22544 non-null  int64  \n",
            " 23  srv_count                    22544 non-null  int64  \n",
            " 24  serror_rate                  22544 non-null  float64\n",
            " 25  srv_serror_rate              22544 non-null  float64\n",
            " 26  rerror_rate                  22544 non-null  float64\n",
            " 27  srv_rerror_rate              22544 non-null  float64\n",
            " 28  same_srv_rate                22544 non-null  float64\n",
            " 29  diff_srv_rate                22544 non-null  float64\n",
            " 30  srv_diff_host_rate           22544 non-null  float64\n",
            " 31  dst_host_count               22544 non-null  int64  \n",
            " 32  dst_host_srv_count           22544 non-null  int64  \n",
            " 33  dst_host_same_srv_rate       22544 non-null  float64\n",
            " 34  dst_host_diff_srv_rate       22544 non-null  float64\n",
            " 35  dst_host_same_src_port_rate  22544 non-null  float64\n",
            " 36  dst_host_srv_diff_host_rate  22544 non-null  float64\n",
            " 37  dst_host_serror_rate         22544 non-null  float64\n",
            " 38  dst_host_srv_serror_rate     22544 non-null  float64\n",
            " 39  dst_host_rerror_rate         22544 non-null  float64\n",
            " 40  dst_host_srv_rerror_rate     22544 non-null  float64\n",
            " 41  class                        22544 non-null  object \n",
            "dtypes: float64(15), int64(23), object(4)\n",
            "memory usage: 7.2+ MB\n",
            "\n",
            "--- Training Data Descriptive Statistics ---\n",
            "           duration     src_bytes     dst_bytes           land  \\\n",
            "count  125973.00000  1.259730e+05  1.259730e+05  125973.000000   \n",
            "mean      287.14465  4.556674e+04  1.977911e+04       0.000198   \n",
            "std      2604.51531  5.870331e+06  4.021269e+06       0.014086   \n",
            "min         0.00000  0.000000e+00  0.000000e+00       0.000000   \n",
            "25%         0.00000  0.000000e+00  0.000000e+00       0.000000   \n",
            "50%         0.00000  4.400000e+01  0.000000e+00       0.000000   \n",
            "75%         0.00000  2.760000e+02  5.160000e+02       0.000000   \n",
            "max     42908.00000  1.379964e+09  1.309937e+09       1.000000   \n",
            "\n",
            "       wrong_fragment         urgent            hot  num_failed_logins  \\\n",
            "count   125973.000000  125973.000000  125973.000000      125973.000000   \n",
            "mean         0.022687       0.000111       0.204409           0.001222   \n",
            "std          0.253530       0.014366       2.149968           0.045239   \n",
            "min          0.000000       0.000000       0.000000           0.000000   \n",
            "25%          0.000000       0.000000       0.000000           0.000000   \n",
            "50%          0.000000       0.000000       0.000000           0.000000   \n",
            "75%          0.000000       0.000000       0.000000           0.000000   \n",
            "max          3.000000       3.000000      77.000000           5.000000   \n",
            "\n",
            "           logged_in  num_compromised  ...  dst_host_count  \\\n",
            "count  125973.000000    125973.000000  ...   125973.000000   \n",
            "mean        0.395736         0.279250  ...      182.148945   \n",
            "std         0.489010        23.942042  ...       99.206213   \n",
            "min         0.000000         0.000000  ...        0.000000   \n",
            "25%         0.000000         0.000000  ...       82.000000   \n",
            "50%         0.000000         0.000000  ...      255.000000   \n",
            "75%         1.000000         0.000000  ...      255.000000   \n",
            "max         1.000000      7479.000000  ...      255.000000   \n",
            "\n",
            "       dst_host_srv_count  dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
            "count       125973.000000           125973.000000           125973.000000   \n",
            "mean           115.653005                0.521242                0.082951   \n",
            "std            110.702741                0.448949                0.188922   \n",
            "min              0.000000                0.000000                0.000000   \n",
            "25%             10.000000                0.050000                0.000000   \n",
            "50%             63.000000                0.510000                0.020000   \n",
            "75%            255.000000                1.000000                0.070000   \n",
            "max            255.000000                1.000000                1.000000   \n",
            "\n",
            "       dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
            "count                125973.000000                125973.000000   \n",
            "mean                      0.148379                     0.032542   \n",
            "std                       0.308997                     0.112564   \n",
            "min                       0.000000                     0.000000   \n",
            "25%                       0.000000                     0.000000   \n",
            "50%                       0.000000                     0.000000   \n",
            "75%                       0.060000                     0.020000   \n",
            "max                       1.000000                     1.000000   \n",
            "\n",
            "       dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
            "count         125973.000000             125973.000000         125973.000000   \n",
            "mean               0.284452                  0.278485              0.118832   \n",
            "std                0.444784                  0.445669              0.306557   \n",
            "min                0.000000                  0.000000              0.000000   \n",
            "25%                0.000000                  0.000000              0.000000   \n",
            "50%                0.000000                  0.000000              0.000000   \n",
            "75%                1.000000                  1.000000              0.000000   \n",
            "max                1.000000                  1.000000              1.000000   \n",
            "\n",
            "       dst_host_srv_rerror_rate  \n",
            "count             125973.000000  \n",
            "mean                   0.120240  \n",
            "std                    0.319459  \n",
            "min                    0.000000  \n",
            "25%                    0.000000  \n",
            "50%                    0.000000  \n",
            "75%                    0.000000  \n",
            "max                    1.000000  \n",
            "\n",
            "[8 rows x 38 columns]\n",
            "\n",
            "--- Testing Data Descriptive Statistics ---\n",
            "           duration     src_bytes     dst_bytes          land  wrong_fragment  \\\n",
            "count  22544.000000  2.254400e+04  2.254400e+04  22544.000000    22544.000000   \n",
            "mean     218.859076  1.039545e+04  2.056019e+03      0.000311        0.008428   \n",
            "std     1407.176612  4.727864e+05  2.121930e+04      0.017619        0.142599   \n",
            "min        0.000000  0.000000e+00  0.000000e+00      0.000000        0.000000   \n",
            "25%        0.000000  0.000000e+00  0.000000e+00      0.000000        0.000000   \n",
            "50%        0.000000  5.400000e+01  4.600000e+01      0.000000        0.000000   \n",
            "75%        0.000000  2.870000e+02  6.010000e+02      0.000000        0.000000   \n",
            "max    57715.000000  6.282565e+07  1.345927e+06      1.000000        3.000000   \n",
            "\n",
            "             urgent           hot  num_failed_logins     logged_in  \\\n",
            "count  22544.000000  22544.000000       22544.000000  22544.000000   \n",
            "mean       0.000710      0.105394           0.021647      0.442202   \n",
            "std        0.036473      0.928428           0.150328      0.496659   \n",
            "min        0.000000      0.000000           0.000000      0.000000   \n",
            "25%        0.000000      0.000000           0.000000      0.000000   \n",
            "50%        0.000000      0.000000           0.000000      0.000000   \n",
            "75%        0.000000      0.000000           0.000000      1.000000   \n",
            "max        3.000000    101.000000           4.000000      1.000000   \n",
            "\n",
            "       num_compromised  ...  dst_host_count  dst_host_srv_count  \\\n",
            "count     22544.000000  ...    22544.000000        22544.000000   \n",
            "mean          0.119899  ...      193.869411          140.750532   \n",
            "std           7.269597  ...       94.035663          111.783972   \n",
            "min           0.000000  ...        0.000000            0.000000   \n",
            "25%           0.000000  ...      121.000000           15.000000   \n",
            "50%           0.000000  ...      255.000000          168.000000   \n",
            "75%           0.000000  ...      255.000000          255.000000   \n",
            "max         796.000000  ...      255.000000          255.000000   \n",
            "\n",
            "       dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
            "count            22544.000000            22544.000000   \n",
            "mean                 0.608722                0.090540   \n",
            "std                  0.435688                0.220717   \n",
            "min                  0.000000                0.000000   \n",
            "25%                  0.070000                0.000000   \n",
            "50%                  0.920000                0.010000   \n",
            "75%                  1.000000                0.060000   \n",
            "max                  1.000000                1.000000   \n",
            "\n",
            "       dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
            "count                 22544.000000                 22544.000000   \n",
            "mean                      0.132261                     0.019638   \n",
            "std                       0.306268                     0.085394   \n",
            "min                       0.000000                     0.000000   \n",
            "25%                       0.000000                     0.000000   \n",
            "50%                       0.000000                     0.000000   \n",
            "75%                       0.030000                     0.010000   \n",
            "max                       1.000000                     1.000000   \n",
            "\n",
            "       dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
            "count          22544.000000              22544.000000          22544.000000   \n",
            "mean               0.097814                  0.099426              0.233385   \n",
            "std                0.273139                  0.281866              0.387229   \n",
            "min                0.000000                  0.000000              0.000000   \n",
            "25%                0.000000                  0.000000              0.000000   \n",
            "50%                0.000000                  0.000000              0.000000   \n",
            "75%                0.000000                  0.000000              0.360000   \n",
            "max                1.000000                  1.000000              1.000000   \n",
            "\n",
            "       dst_host_srv_rerror_rate  \n",
            "count              22544.000000  \n",
            "mean                   0.226683  \n",
            "std                    0.400875  \n",
            "min                    0.000000  \n",
            "25%                    0.000000  \n",
            "50%                    0.000000  \n",
            "75%                    0.170000  \n",
            "max                    1.000000  \n",
            "\n",
            "[8 rows x 38 columns]\n",
            "\n",
            "--- Missing Values in Training Data ---\n",
            "duration                       0\n",
            "protocol_type                  0\n",
            "service                        0\n",
            "flag                           0\n",
            "src_bytes                      0\n",
            "dst_bytes                      0\n",
            "land                           0\n",
            "wrong_fragment                 0\n",
            "urgent                         0\n",
            "hot                            0\n",
            "num_failed_logins              0\n",
            "logged_in                      0\n",
            "num_compromised                0\n",
            "root_shell                     0\n",
            "su_attempted                   0\n",
            "num_root                       0\n",
            "num_file_creations             0\n",
            "num_shells                     0\n",
            "num_access_files               0\n",
            "num_outbound_cmds              0\n",
            "is_host_login                  0\n",
            "is_guest_login                 0\n",
            "count                          0\n",
            "srv_count                      0\n",
            "serror_rate                    0\n",
            "srv_serror_rate                0\n",
            "rerror_rate                    0\n",
            "srv_rerror_rate                0\n",
            "same_srv_rate                  0\n",
            "diff_srv_rate                  0\n",
            "srv_diff_host_rate             0\n",
            "dst_host_count                 0\n",
            "dst_host_srv_count             0\n",
            "dst_host_same_srv_rate         0\n",
            "dst_host_diff_srv_rate         0\n",
            "dst_host_same_src_port_rate    0\n",
            "dst_host_srv_diff_host_rate    0\n",
            "dst_host_serror_rate           0\n",
            "dst_host_srv_serror_rate       0\n",
            "dst_host_rerror_rate           0\n",
            "dst_host_srv_rerror_rate       0\n",
            "class                          0\n",
            "dtype: int64\n",
            "\n",
            "--- Missing Values in Testing Data ---\n",
            "duration                       0\n",
            "protocol_type                  0\n",
            "service                        0\n",
            "flag                           0\n",
            "src_bytes                      0\n",
            "dst_bytes                      0\n",
            "land                           0\n",
            "wrong_fragment                 0\n",
            "urgent                         0\n",
            "hot                            0\n",
            "num_failed_logins              0\n",
            "logged_in                      0\n",
            "num_compromised                0\n",
            "root_shell                     0\n",
            "su_attempted                   0\n",
            "num_root                       0\n",
            "num_file_creations             0\n",
            "num_shells                     0\n",
            "num_access_files               0\n",
            "num_outbound_cmds              0\n",
            "is_host_login                  0\n",
            "is_guest_login                 0\n",
            "count                          0\n",
            "srv_count                      0\n",
            "serror_rate                    0\n",
            "srv_serror_rate                0\n",
            "rerror_rate                    0\n",
            "srv_rerror_rate                0\n",
            "same_srv_rate                  0\n",
            "diff_srv_rate                  0\n",
            "srv_diff_host_rate             0\n",
            "dst_host_count                 0\n",
            "dst_host_srv_count             0\n",
            "dst_host_same_srv_rate         0\n",
            "dst_host_diff_srv_rate         0\n",
            "dst_host_same_src_port_rate    0\n",
            "dst_host_srv_diff_host_rate    0\n",
            "dst_host_serror_rate           0\n",
            "dst_host_srv_serror_rate       0\n",
            "dst_host_rerror_rate           0\n",
            "dst_host_srv_rerror_rate       0\n",
            "class                          0\n",
            "dtype: int64\n",
            "\n",
            "--- Unique Values in Categorical Columns (Training Data) ---\n",
            "Column 'protocol_type': 3 unique values\n",
            "protocol_type\n",
            "tcp     102689\n",
            "udp      14993\n",
            "icmp      8291\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Column 'service': 70 unique values\n",
            "service\n",
            "http         40338\n",
            "private      21853\n",
            "domain_u      9043\n",
            "smtp          7313\n",
            "ftp_data      6860\n",
            "             ...  \n",
            "tftp_u           3\n",
            "http_8001        2\n",
            "aol              2\n",
            "harvest          2\n",
            "http_2784        1\n",
            "Name: count, Length: 70, dtype: int64\n",
            "\n",
            "\n",
            "Column 'flag': 11 unique values\n",
            "flag\n",
            "SF        74945\n",
            "S0        34851\n",
            "REJ       11233\n",
            "RSTR       2421\n",
            "RSTO       1562\n",
            "S1          365\n",
            "SH          271\n",
            "S2          127\n",
            "RSTOS0      103\n",
            "S3           49\n",
            "OTH          46\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Column 'class': 23 unique values\n",
            "class\n",
            "normal             67343\n",
            "neptune            41214\n",
            "satan               3633\n",
            "ipsweep             3599\n",
            "portsweep           2931\n",
            "smurf               2646\n",
            "nmap                1493\n",
            "back                 956\n",
            "teardrop             892\n",
            "warezclient          890\n",
            "pod                  201\n",
            "guess_passwd          53\n",
            "buffer_overflow       30\n",
            "warezmaster           20\n",
            "land                  18\n",
            "imap                  11\n",
            "rootkit               10\n",
            "loadmodule             9\n",
            "ftp_write              8\n",
            "multihop               7\n",
            "phf                    4\n",
            "perl                   3\n",
            "spy                    2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "\n",
            "--- Unique Values in Categorical Columns (Testing Data) ---\n",
            "Column 'protocol_type': 3 unique values\n",
            "protocol_type\n",
            "tcp     18880\n",
            "udp      2621\n",
            "icmp     1043\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Column 'service': 64 unique values\n",
            "service\n",
            "http       7853\n",
            "private    4774\n",
            "telnet     1626\n",
            "pop_3      1019\n",
            "smtp        934\n",
            "           ... \n",
            "printer      11\n",
            "ntp_u        10\n",
            "rje           8\n",
            "tim_i         6\n",
            "tftp_u        1\n",
            "Name: count, Length: 64, dtype: int64\n",
            "\n",
            "\n",
            "Column 'flag': 11 unique values\n",
            "flag\n",
            "SF        14875\n",
            "REJ        3850\n",
            "S0         2013\n",
            "RSTO        773\n",
            "RSTR        669\n",
            "S3          249\n",
            "SH           73\n",
            "S1           21\n",
            "S2           15\n",
            "OTH           4\n",
            "RSTOS0        2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Column 'class': 38 unique values\n",
            "class\n",
            "normal             9711\n",
            "neptune            4657\n",
            "guess_passwd       1231\n",
            "mscan               996\n",
            "warezmaster         944\n",
            "apache2             737\n",
            "satan               735\n",
            "processtable        685\n",
            "smurf               665\n",
            "back                359\n",
            "snmpguess           331\n",
            "saint               319\n",
            "mailbomb            293\n",
            "snmpgetattack       178\n",
            "portsweep           157\n",
            "ipsweep             141\n",
            "httptunnel          133\n",
            "nmap                 73\n",
            "pod                  41\n",
            "buffer_overflow      20\n",
            "multihop             18\n",
            "named                17\n",
            "ps                   15\n",
            "sendmail             14\n",
            "xterm                13\n",
            "rootkit              13\n",
            "teardrop             12\n",
            "xlock                 9\n",
            "land                  7\n",
            "xsnoop                4\n",
            "ftp_write             3\n",
            "loadmodule            2\n",
            "worm                  2\n",
            "perl                  2\n",
            "sqlattack             2\n",
            "udpstorm              2\n",
            "phf                   2\n",
            "imap                  1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70f24e5c"
      },
      "source": [
        "```markdown\n",
        "## Feature Engineering: Handling Categorical Features\n",
        "\n",
        "Based on the initial data exploration, we have identified several categorical features: `protocol_type`, `service`, `flag`, and `class`. For machine learning models, these categorical features need to be converted into a numerical format.\n",
        "\n",
        "We will use **one-hot encoding** for the `protocol_type`, `service`, and `flag` columns. One-hot encoding is suitable for nominal categorical variables where there is no inherent order between the categories. It creates new binary features for each category, which helps prevent the model from misinterpreting arbitrary numerical assignments as ordinal relationships.\n",
        "\n",
        "For the `class` column, which contains the attack types, we will first simplify it into a binary classification problem: 'normal' vs. 'attack'. This is a common approach in intrusion detection systems to first distinguish benign traffic from malicious traffic. We will then use label encoding for this binary target variable.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e6d4887"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the strategy outlined in the previous markdown block, I will now apply one-hot encoding to the categorical feature columns (`protocol_type`, `service`, `flag`) in both training and testing datasets. I will then create a binary target variable from the 'class' column, mapping 'normal' to 0 and all attack types to 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a59b036",
        "outputId": "2055c900-fe8a-4df7-a151-20fa900a89ec"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Identify categorical feature columns for one-hot encoding\n",
        "categorical_features = ['protocol_type', 'service', 'flag']\n",
        "\n",
        "# Apply one-hot encoding to training data\n",
        "train_df_encoded = pd.get_dummies(train_df, columns=categorical_features, drop_first=False)\n",
        "print(f\"Shape of training data after one-hot encoding: {train_df_encoded.shape}\")\n",
        "\n",
        "# Apply one-hot encoding to testing data\n",
        "test_df_encoded = pd.get_dummies(test_df, columns=categorical_features, drop_first=False)\n",
        "print(f\"Shape of testing data after one-hot encoding: {test_df_encoded.shape}\")\n",
        "\n",
        "# Align columns between training and testing data after one-hot encoding\n",
        "# This is crucial as test set might not have all categories present in train set or vice-versa\n",
        "common_cols = list(set(train_df_encoded.columns) & set(test_df_encoded.columns))\n",
        "train_df_encoded = train_df_encoded[common_cols]\n",
        "test_df_encoded = test_df_encoded[common_cols]\n",
        "\n",
        "# Add missing columns to test_df_encoded and fill with 0\n",
        "missing_in_test = set(train_df_encoded.columns) - set(test_df_encoded.columns)\n",
        "for c in missing_in_test:\n",
        "    test_df_encoded[c] = 0\n",
        "\n",
        "# Add missing columns to train_df_encoded and fill with 0\n",
        "missing_in_train = set(test_df_encoded.columns) - set(train_df_encoded.columns)\n",
        "for c in missing_in_train:\n",
        "    train_df_encoded[c] = 0\n",
        "\n",
        "# Ensure the order of columns is the same\n",
        "test_df_encoded = test_df_encoded[train_df_encoded.columns]\n",
        "\n",
        "print(f\"Shape of aligned training data: {train_df_encoded.shape}\")\n",
        "print(f\"Shape of aligned testing data: {test_df_encoded.shape}\")\n",
        "\n",
        "# Convert the 'class' column into a binary target variable (0 for normal, 1 for attack)\n",
        "# First, check unique values in the 'class' column\n",
        "print(\"\\nUnique values in 'class' column (training data):\", train_df['class'].unique())\n",
        "\n",
        "# Define a function to map attack types to 1 and normal to 0\n",
        "def map_class_to_binary(df_class):\n",
        "    return df_class.apply(lambda x: 0 if x == 'normal' else 1)\n",
        "\n",
        "# Apply the mapping to both training and testing 'class' columns\n",
        "y_train_binary = map_class_to_binary(train_df_encoded['class'])\n",
        "y_test_binary = map_class_to_binary(test_df_encoded['class'])\n",
        "\n",
        "# Drop the original 'class' column from the encoded dataframes\n",
        "X_train_encoded = train_df_encoded.drop('class', axis=1)\n",
        "X_test_encoded = test_df_encoded.drop('class', axis=1)\n",
        "\n",
        "print(\"\\nBinary target variable created. First 5 values for y_train_binary:\")\n",
        "print(y_train_binary.head())\n",
        "print(\"First 5 values for y_test_binary:\")\n",
        "print(y_test_binary.head())\n",
        "\n",
        "print(f\"Shape of X_train_encoded: {X_train_encoded.shape}\")\n",
        "print(f\"Shape of X_test_encoded: {X_test_encoded.shape}\")\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training data after one-hot encoding: (125973, 123)\n",
            "Shape of testing data after one-hot encoding: (22544, 117)\n",
            "Shape of aligned training data: (125973, 117)\n",
            "Shape of aligned testing data: (22544, 117)\n",
            "\n",
            "Unique values in 'class' column (training data): ['normal' 'neptune' 'warezclient' 'ipsweep' 'portsweep' 'teardrop' 'nmap'\n",
            " 'satan' 'smurf' 'pod' 'back' 'guess_passwd' 'ftp_write' 'multihop'\n",
            " 'rootkit' 'buffer_overflow' 'imap' 'warezmaster' 'phf' 'land'\n",
            " 'loadmodule' 'spy' 'perl']\n",
            "\n",
            "Binary target variable created. First 5 values for y_train_binary:\n",
            "0    0\n",
            "1    0\n",
            "2    1\n",
            "3    0\n",
            "4    0\n",
            "Name: class, dtype: int64\n",
            "First 5 values for y_test_binary:\n",
            "0    1\n",
            "1    1\n",
            "2    0\n",
            "3    1\n",
            "4    1\n",
            "Name: class, dtype: int64\n",
            "Shape of X_train_encoded: (125973, 116)\n",
            "Shape of X_test_encoded: (22544, 116)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cfc8265"
      },
      "source": [
        "```markdown\n",
        "## Feature Scaling: Normalizing Numerical Features\n",
        "\n",
        "After handling categorical features, the next crucial preprocessing step is to scale the numerical features. Many machine learning algorithms perform better or converge faster when features are on a similar scale. Features with a larger range of values might unduly influence the model's performance.\n",
        "\n",
        "We will use **MinMaxScaler** to normalize the numerical features. This scaler transforms features by scaling each feature to a given range, typically between zero and one. This transformation is applied to all numerical columns, excluding the one-hot encoded features and the target variable.\n",
        "\n",
        "It's important to fit the `MinMaxScaler` only on the training data and then use the *same fitted scaler* to transform both the training and testing data. This prevents data leakage from the test set into the training process.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ec7ff31"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the previous step of handling categorical features, I will now normalize the numerical features in both the training and testing datasets using `MinMaxScaler`. This will scale the numerical features to a range between 0 and 1, preventing features with larger values from dominating the learning process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "dbe95f2e",
        "outputId": "417c4fa4-c2c6-48d2-abe5-0411e1732893"
      },
      "source": [
        "```markdown\n",
        "## Feature Scaling: Normalizing Numerical Features\n",
        "\n",
        "After handling categorical features, the next crucial preprocessing step is to scale the numerical features. Many machine learning algorithms perform better or converge faster when features are on a similar scale. Features with a larger range of values might unduly influence the model's performance.\n",
        "\n",
        "We will use **MinMaxScaler** to normalize the numerical features. This scaler transforms features by scaling each feature to a given range, typically between zero and one. This transformation is applied to all numerical columns, excluding the one-hot encoded features and the target variable.\n",
        "\n",
        "It's important to fit the `MinMaxScaler` only on the training data and then use the *same fitted scaler* to transform both the training and testing data. This prevents data leakage from the test set into the training process."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 4) (ipython-input-1368824199.py, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1368824199.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    After handling categorical features, the next crucial preprocessing step is to scale the numerical features. Many machine learning algorithms perform better or converge faster when features are on a similar scale. Features with a larger range of values might unduly influence the model's performance.\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb12c202"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block contained markdown text within a Python cell, leading to a `SyntaxError`. The intention was to display a markdown block, not execute Python code. I am correcting this by generating the markdown block as intended.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c70a8f6"
      },
      "source": [
        "```markdown\n",
        "## Feature Scaling: Normalizing Numerical Features\n",
        "\n",
        "After handling categorical features, the next crucial preprocessing step is to scale the numerical features. Many machine learning algorithms perform better or converge faster when features are on a similar scale. Features with a larger range of values might unduly influence the model's performance.\n",
        "\n",
        "We will use **MinMaxScaler** to normalize the numerical features. This scaler transforms features by scaling each feature to a given range, typically between zero and one. This transformation is applied to all numerical columns, excluding the one-hot encoded features and the target variable.\n",
        "\n",
        "It's important to fit the `MinMaxScaler` only on the training data and then use the *same fitted scaler* to transform both the training and testing data. This prevents data leakage from the test set into the training process."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9608dbf"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to display the markdown block on feature scaling resulted in a `SyntaxError` because the markdown content was incorrectly placed within a Python code cell. I am correcting this by properly generating a `markdown_block` command with the correct `cell_type` to display the explanatory text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3562fa5"
      },
      "source": [
        "## Feature Scaling: Normalizing Numerical Features\n",
        "\n",
        "After handling categorical features, the next crucial preprocessing step is to scale the numerical features. Many machine learning algorithms perform better or converge faster when features are on a similar scale. Features with a larger range of values might unduly influence the model's performance.\n",
        "\n",
        "We will use **MinMaxScaler** to normalize the numerical features. This scaler transforms features by scaling each feature to a given range, typically between zero and one. This transformation is applied to all numerical columns, excluding the one-hot encoded features and the target variable.\n",
        "\n",
        "It's important to fit the `MinMaxScaler` only on the training data and then use the *same fitted scaler* to transform both the training and testing data. This prevents data leakage from the test set into the training process."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0c38859"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block contained markdown text within a Python cell, leading to a `SyntaxError`. The intention was to display a markdown block, not execute Python code. I am correcting this by generating a `markdown_block` command with the correct `cell_type` to display the explanatory text. After this, I will add the actual Python code for feature scaling using `MinMaxScaler`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9b61861"
      },
      "source": [
        "## Feature Scaling: Normalizing Numerical Features\n",
        "\n",
        "After handling categorical features, the next crucial preprocessing step is to scale the numerical features. Many machine learning algorithms perform better or converge faster when features are on a similar scale. Features with a larger range of values might unduly influence the model's performance.\n",
        "\n",
        "We will use **MinMaxScaler** to normalize the numerical features. This scaler transforms features by scaling each feature to a given range, typically between zero and one. This transformation is applied to all numerical columns, excluding the one-hot encoded features and the target variable.\n",
        "\n",
        "It's important to fit the `MinMaxScaler` only on the training data and then use the *same fitted scaler* to transform both the training and testing data. This prevents data leakage from the test set into the training process."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e0fe442"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block contained markdown text within a Python cell, which caused a `SyntaxError`. To fix this, I will generate a proper `markdown_block` command to display the explanatory text about feature scaling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03770081"
      },
      "source": [
        "## Feature Scaling: Normalizing Numerical Features\n",
        "\n",
        "After handling categorical features, the next crucial preprocessing step is to scale the numerical features. Many machine learning algorithms perform better or converge faster when features are on a similar scale. Features with a larger range of values might unduly influence the model's performance.\n",
        "\n",
        "We will use **MinMaxScaler** to normalize the numerical features. This scaler transforms features by scaling each feature to a given range, typically between zero and one. This transformation is applied to all numerical columns, excluding the one-hot encoded features and the target variable.\n",
        "\n",
        "It's important to fit the `MinMaxScaler` only on the training data and then use the *same fitted scaler* to transform both the training and testing data. This prevents data leakage from the test set into the training process."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88735fd8"
      },
      "source": [
        "## Feature Scaling: Normalizing Numerical Features (Retry)\n",
        "\n",
        "### Subtask:\n",
        "Normalize the numerical features in the training and testing datasets using `MinMaxScaler`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c35c2402"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now implement the feature scaling using `MinMaxScaler` as described in the instructions, fitting the scaler on the training data and then transforming both training and testing datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2b66000"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
        "X_test_scaled = scaler.transform(X_test_encoded)\n",
        "\n",
        "# Display the shape of the scaled datasets\n",
        "print(f\"Shape of X_train_scaled: {X_train_scaled.shape}\")\n",
        "print(f\"Shape of X_test_scaled: {X_test_scaled.shape}\")\n",
        "\n",
        "print(\"Feature scaling completed using MinMaxScaler.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cf53495"
      },
      "source": [
        "## Finalize Data Preparation for Deep Learning Model\n",
        "\n",
        "### Subtask:\n",
        "Convert target variables to NumPy arrays and split the training data into training and validation sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0fbbf3c"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now convert the target variables to NumPy arrays and then split the training data into training and validation sets as instructed, ensuring to print the shapes of all resulting datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24b5b7ec"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Convert y_train_binary and y_test_binary to NumPy arrays\n",
        "y_train = y_train_binary.to_numpy()\n",
        "y_test = y_test_binary.to_numpy()\n",
        "\n",
        "print(f\"Converted y_train_binary to y_train (NumPy array) shape: {y_train.shape}\")\n",
        "print(f\"Converted y_test_binary to y_test (NumPy array) shape: {y_test.shape}\")\n",
        "\n",
        "# 2. Split X_train_scaled and y_train into training and validation sets (80% train, 20% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "# 3. Print the shapes of the final datasets\n",
        "print(\"\\n--- Final Data Shapes ---\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}\")\n",
        "print(f\"y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test_scaled.shape}\") # X_test is already X_test_scaled\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "print(\"\\nData preparation for deep learning model finalized.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2570865"
      },
      "source": [
        "## Deep Learning Model Development\n",
        "\n",
        "### Subtask:\n",
        "Design a suitable deep learning architecture for binary classification of network traffic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cd49221"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to design a deep learning model. Following the instructions, I will first import the necessary modules from TensorFlow/Keras, then define a sequential model with Dense and Dropout layers for binary classification, and finally compile and summarize the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2259dc8d"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# 1. Get the number of features from X_train to define input shape\n",
        "input_dim = X_train.shape[1]\n",
        "print(f\"Number of features (input_dim): {input_dim}\")\n",
        "\n",
        "# 2. Initialize a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# 3. Add an input layer (first Dense layer)\n",
        "model.add(Dense(128, activation='relu', input_shape=(input_dim,)))\n",
        "\n",
        "# 4. Add hidden Dense layers with Dropout\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# 5. Add an output Dense layer for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 6. Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 7. Print the model summary\n",
        "print(\"\\n--- Model Architecture Summary ---\")\n",
        "model.summary()\n",
        "\n",
        "print(\"Deep learning model architecture designed and compiled.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac44ac73"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code produced a `UserWarning` from Keras regarding the `input_shape` argument. To align with Keras best practices and remove the warning, I will modify the model definition to explicitly use an `Input` layer as the first layer in the `Sequential` model instead of passing `input_shape` to the first `Dense` layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c287c74f"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "\n",
        "# 1. Get the number of features from X_train to define input shape\n",
        "input_dim = X_train.shape[1]\n",
        "print(f\"Number of features (input_dim): {input_dim}\")\n",
        "\n",
        "# 2. Initialize a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# 3. Add an Input layer (recommended for Sequential models)\n",
        "model.add(Input(shape=(input_dim,)))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# 4. Add hidden Dense layers with Dropout\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# 5. Add an output Dense layer for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 6. Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 7. Print the model summary\n",
        "print(\"\\n--- Model Architecture Summary ---\")\n",
        "model.summary()\n",
        "\n",
        "print(\"Deep learning model architecture designed and compiled.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3658acd"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "### Subtask:\n",
        "Train the deep learning model using the preprocessed dataset, optimizing its parameters to effectively learn patterns of normal and anomalous network behavior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18e67037"
      },
      "source": [
        "**Reasoning**:\n",
        "I will import necessary callbacks, define early stopping and model checkpoint, and then train the model using the prepared data and callbacks as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "040ccabb"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# 1. Define EarlyStopping callback\n",
        "# Monitor 'val_loss', with patience of 5 epochs, and restore best weights found during training\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# 2. Define ModelCheckpoint callback\n",
        "# Save the best model weights based on 'val_loss', only saving the best model\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_model.weights.h5', # Filepath to save the model weights\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True, # We only need to save weights, not the entire model architecture\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 3. Train the model\n",
        "# Use X_train, y_train for training and X_val, y_val for validation\n",
        "# Set epochs to 50 and batch_size to 128 as examples.\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Model training completed. Training history stored in 'history'.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}